import os
import tempfile
import streamlit as st
from dotenv import load_dotenv

# === LangChain æ–°ç»“æž„ ===
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from pypdf import PdfReader
import re

# ===== åˆå§‹åŒ– =====
load_dotenv()

def fix_page_number_in_content(content, correct_page):
    """ä¿®å¤æ–‡æ¡£å†…å®¹ä¸­çš„é¡µç ä¿¡æ¯"""
    # æŸ¥æ‰¾å¹¶æ›¿æ¢ "Apple Inc. | 2024 Form 10-K | X" æ ¼å¼çš„é¡µç 
    pattern = r'Apple Inc\. \| 2024 Form 10-K \| \d+'
    replacement = f'Apple Inc. | 2024 Form 10-K | {correct_page}'
    return re.sub(pattern, replacement, content)

INDEX_DIR = "faiss_index"
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

st.set_page_config(page_title="ðŸ¢ Enterprise Knowledge QA System", page_icon="ðŸ¢")
st.title("ðŸ¢ Enterprise Knowledge QA System")
st.markdown("Upload enterprise documents (e.g., annual reports, policies) and ask questions interactively.")

# ===== åˆå§‹åŒ–æ¨¡åž‹ =====
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# ===== ä¸Šä¼ æ–‡ä»¶ =====
uploaded_files = st.file_uploader("ðŸ“„ Upload PDF files", type=["pdf"], accept_multiple_files=True)

if uploaded_files:
    all_docs = []
    page_mapping = {}  # å­˜å‚¨æ–‡æ¡£ç´¢å¼•åˆ°é¡µç çš„æ˜ å°„
    with st.spinner("Processing PDFs..."):
        for file_idx, uploaded_file in enumerate(uploaded_files):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_path = tmp_file.name

            # èŽ·å–PDFæ€»é¡µæ•°
            reader = PdfReader(tmp_path)
            total_pages = len(reader.pages)
            
            loader = PyPDFLoader(tmp_path)
            docs = loader.load()
            
            # è®¡ç®—é¡µç åç§»é‡
            page_offset = 0
            offset_found = False
            
            # æŸ¥æ‰¾æ–‡æ¡£å†…å®¹ä¸­çš„é¡µç æ¨¡å¼æ¥ç¡®å®šåç§»é‡
            for doc in docs:
                content = doc.page_content
                # æŸ¥æ‰¾ "Apple Inc. | 2024 Form 10-K | X" æ¨¡å¼
                match = re.search(r'Apple Inc\. \| 2024 Form 10-K \| (\d+)', content)
                if match:
                    content_page = int(match.group(1))
                    loader_page = doc.metadata.get('page', 0)
                    # è®¡ç®—åç§»é‡ï¼šå†…å®¹é¡µç  - loaderé¡µç 
                    page_offset = (content_page - 1) - loader_page
                    offset_found = True
                    break
            
            if not offset_found:
                page_offset = 0
            
            # ä¸ºæ¯ä¸ªåŽŸå§‹æ–‡æ¡£æ·»åŠ æ–‡ä»¶åå’Œé¡µç ä¿¡æ¯
            for doc in docs:
                doc.metadata['filename'] = uploaded_file.name
                doc.metadata['total_pages'] = total_pages
                # ç¡®ä¿é¡µç ä¿¡æ¯æ­£ç¡®ä¼ é€’
                if 'page' not in doc.metadata:
                    doc.metadata['page'] = 0
            
            splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
            split_docs = splitter.split_documents(docs)
            
            # ä¸ºæ¯ä¸ªæ–‡æ¡£ç‰‡æ®µæ·»åŠ æ–‡ä»¶åå’Œæ­£ç¡®çš„é¡µç æ˜ å°„
            for doc in split_docs:
                # èŽ·å–åŽŸå§‹é¡µç ï¼ˆä»Ž0å¼€å§‹ï¼‰
                original_page = doc.metadata.get('page', 0)
                # åº”ç”¨åç§»é‡å¹¶è½¬æ¢ä¸ºæ‰“å°é¡µç ï¼ˆä»Ž1å¼€å§‹ï¼‰
                corrected_page = original_page + page_offset
                print_page = corrected_page + 1
                
                # ç¡®ä¿æ–‡ä»¶åå’Œé¡µç ä¿¡æ¯æ­£ç¡®ä¼ é€’
                doc.metadata['filename'] = uploaded_file.name
                # Source æ˜¾ç¤ºå°é¡µç ï¼ˆåŽŸå§‹é¡µç ï¼‰
                original_print_page = original_page + 1
                doc.metadata['print_page'] = original_print_page
                doc.metadata['total_pages'] = total_pages
                
                # ä¿®å¤æ–‡æ¡£å†…å®¹ä¸­çš„é¡µç ä¿¡æ¯ - ä½¿ç”¨å¤§é¡µç ï¼ˆç»è¿‡åç§»é‡ä¿®æ­£çš„é¡µç ï¼‰
                doc.page_content = fix_page_number_in_content(doc.page_content, print_page)
                
                # å­˜å‚¨æ˜ å°„å…³ç³»
                doc_id = f"{uploaded_file.name}_{original_page}"
                page_mapping[doc_id] = {
                    'filename': uploaded_file.name,
                    'print_page': print_page,
                    'total_pages': total_pages
                }
                
            
            all_docs.extend(split_docs)

        # å»ºç«‹å‘é‡æ•°æ®åº“
        vectorstore = FAISS.from_documents(all_docs, embeddings)
        vectorstore.save_local(INDEX_DIR)

    st.success("âœ… Documents processed and indexed!")

    # ===== æž„å»º RAG QA é“¾ =====
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    # ===== ç”¨æˆ·æé—® =====
    query = st.text_input("ðŸ’¬ Ask a question about your documents:")
    if query:
        with st.spinner("Generating answer..."):
            # Retrieve relevant documents
            docs = retriever.invoke(query)
            
            # Format context from retrieved documents
            context = "\n\n".join([f"Document {i+1}:\n{doc.page_content}" for i, doc in enumerate(docs)])
            
            # Create prompt and get answer
            prompt = ChatPromptTemplate.from_messages([
                ("system", "You are an enterprise knowledge assistant. Use the provided context to answer the question. If the context contains relevant information, provide a comprehensive answer. If some information is missing, mention what you can provide based on the available context."),
                ("human", "Context:\n{context}\n\nQuestion: {question}")
            ])
            
            chain = prompt | llm | StrOutputParser()
            answer = chain.invoke({"context": context, "question": query})

            st.markdown(f"### ðŸ§  Answer:\n{answer}")

            st.markdown("### ðŸ“š Sources:")
            for i, d in enumerate(docs[:2]):
                # èŽ·å–æ–‡ä»¶åå’Œæ‰“å°é¡µç 
                filename = d.metadata.get('filename', 'Unknown')
                print_page = d.metadata.get('print_page', 'N/A')
                total_pages = d.metadata.get('total_pages', 'N/A')
                
                st.write(f"**Source {i+1}:** {filename} - Page {print_page} of {total_pages}")
                # ä½¿ç”¨ st.text_area æ¥æ˜¾ç¤ºé•¿æ–‡æœ¬ï¼Œå¹¶è®¾ç½®åˆé€‚çš„è¡Œæ•°
                st.text_area(
                    f"Content {i+1}:", 
                    value=d.page_content[:500] + "..." if len(d.page_content) > 500 else d.page_content,
                    height=100,
                    key=f"source_{i}",
                    disabled=True
                )
else:
    st.info("ðŸ‘† Please upload at least one PDF to begin.")


